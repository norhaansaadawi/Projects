{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6izp0vC_W2o-",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install findspark\n"
      ],
      "metadata": {
        "id": "ile0h--xU36i",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "9BL75aRBVT3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression"
      ],
      "metadata": {
        "id": "dqbfgElPXRui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"student performance\").getOrCreate()"
      ],
      "metadata": {
        "id": "Ck8OaVPUXdmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = spark.read.csv(\"/content/Student_performance_data _.csv\", inferSchema = True, header = True)\n",
        "print('There are ',len(dataset.columns),'columns in this dataset')\n",
        "print('There are ',dataset.count(), 'rows in this dataset')\n"
      ],
      "metadata": {
        "id": "lKjXk_-nXl0E",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.show()\n",
        "# dataset previously cleaned before using so this step is skipped, straight to analysis"
      ],
      "metadata": {
        "id": "1hvFJxGPYhfN",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assembler = VectorAssembler(inputCols = ['Age', 'Gender', 'Ethnicity', 'ParentalEducation', 'StudyTimeWeekly', 'Absences', 'Tutoring', 'ParentalSupport', 'Extracurricular','Sports','Music','Volunteering'], outputCol='features')\n",
        "output = assembler.transform(dataset)"
      ],
      "metadata": {
        "id": "s4OBbIobo2-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finalised_data = output.select('features', 'GradeClass')\n",
        "finalised_data.show()"
      ],
      "metadata": {
        "id": "yPSzqx6cplmb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = finalised_data.randomSplit([0.8, 0.2])"
      ],
      "metadata": {
        "id": "9MwK_Pjwp8oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_reg = LogisticRegression(featuresCol = \"features\", labelCol=\"GradeClass\")\n",
        "log_regmodel = log_reg.fit(train)\n",
        "predictions = log_regmodel.transform(test)"
      ],
      "metadata": {
        "id": "IhIVGeKwqBV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "multi_eval = MulticlassClassificationEvaluator(labelCol=\"GradeClass\", predictionCol=\"prediction\")\n",
        "accuracy = multi_eval.evaluate(predictions, {multi_eval.metricName: \"accuracy\"})\n",
        "precision = multi_eval.evaluate(predictions, {multi_eval.metricName: \"weightedPrecision\"})\n",
        "recall = multi_eval.evaluate(predictions, {multi_eval.metricName: \"weightedRecall\"})\n",
        "f1 = multi_eval.evaluate(predictions, {multi_eval.metricName: \"f1\"})"
      ],
      "metadata": {
        "id": "7wLcsDMctDOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "tqsvvr5ptvKG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}